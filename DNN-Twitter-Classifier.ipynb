{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpdRFocItw0Z"
      },
      "source": [
        "#Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uB29idD_t4CU"
      },
      "source": [
        "##Data Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3WLp_yqotc_T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7430a907-aea8-429b-dd17-c1c1b23ccbcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#TrainData\n",
        "file_pathTr = \"/content/drive/My Drive/TP1-NLP/twitter-2013train-A.txt\"\n",
        "column_names = ['Id', 'sentiment', 'text']\n",
        "dataTr = pd.read_csv(file_pathTr, sep='\\t', names=column_names)\n",
        "\n",
        "#DevData\n",
        "file_pathDv = \"/content/drive/My Drive/TP1-NLP/twitter-2013dev-A.txt\"\n",
        "column_names = ['Id', 'sentiment', 'text']\n",
        "dataDv = pd.read_csv(file_pathDv, sep='\\t', names=column_names)\n",
        "\n",
        "#TestData\n",
        "file_pathTst = \"/content/drive/My Drive/TP1-NLP/twitter-2013test-A.txt\"\n",
        "column_names = ['Id', 'sentiment', 'text']\n",
        "dataTst = pd.read_csv(file_pathTst, sep='\\t', names=column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-VdzhUWdrjl"
      },
      "source": [
        "##Normalisation & vocab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UK9Sf6gHdw5t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd9d705c-a7ac-4a16-8a50-c62ddc380420"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "from nltk.stem.snowball import EnglishStemmer\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from collections import Counter\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    # faire la lemmatisation avec WordNetLemmatizer et les stop words de NLTK\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "dataTr['text'] = dataTr['text'].apply(preprocess_text)\n",
        "dataDv['text'] = dataDv['text'].apply(preprocess_text)\n",
        "dataTst['text'] = dataTst['text'].apply(preprocess_text)\n",
        "\n",
        "\n",
        "corpus = ' '.join(dataTr['text'].astype(str).tolist())\n",
        "words = word_tokenize(corpus)\n",
        "\n",
        "word_counts = Counter(words)\n",
        "vocab = {word: i + 1 for i, word in enumerate(word_counts)}\n",
        "\n",
        "vocab_size = len(vocab) + 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4yRqFG4ejEO"
      },
      "source": [
        "##Préparation des Datasets et Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9YNVkN9ekv9"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "class TwitterDataset(Dataset):\n",
        "    def __init__(self, dataframe, vocab):\n",
        "        self.dataframe = dataframe\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.dataframe.iloc[idx]['text']\n",
        "        sentiment = self.dataframe.iloc[idx]['sentiment']\n",
        "        sentiment_to_idx = {'positive': 0, 'negative': 1, 'neutral': 2}\n",
        "        encoded_sentiment = sentiment_to_idx[sentiment]\n",
        "        encoded_text = [self.vocab.get(word, 0) for word in text.split()]\n",
        "        return torch.tensor(encoded_text, dtype=torch.long), torch.tensor(encoded_sentiment, dtype=torch.long)\n",
        "\n",
        "# Création des datasets\n",
        "train_dataset = TwitterDataset(dataTr, vocab)\n",
        "dev_dataset = TwitterDataset(dataDv, vocab)\n",
        "test_dataset = TwitterDataset(dataTst, vocab)\n",
        "\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "def collate_batch(batch):\n",
        "    data, labels = zip(*batch)\n",
        "    data = pad_sequence(data, batch_first=True, padding_value=0)\n",
        "    labels = torch.tensor(labels, dtype=torch.long)\n",
        "    return data, labels\n",
        "\n",
        "\n",
        "\n",
        "# Création des dataloaders\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, collate_fn=collate_batch)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, collate_fn=collate_batch)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVykMq73eqnC"
      },
      "source": [
        "##Construction du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IiAprWh1er-S"
      },
      "outputs": [],
      "source": [
        "class SentimentDNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim=3):\n",
        "        super(SentimentDNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.fc1 = nn.Linear(embedding_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# Création du modèle DNN\n",
        "vocab_size = len(vocab) + 1\n",
        "embedding_dim = 120\n",
        "hidden_dim = 136\n",
        "output_dim = 3\n",
        "\n",
        "model = SentimentDNN(vocab_size, embedding_dim, hidden_dim, output_dim)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rIbY_GRRQne"
      },
      "source": [
        "##Entraînement et Évaluation du modèle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XH_wM7R4RSCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77378480-8342-4839-ee2b-6b55dd4842d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rapport de classification pour les données de développement:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.58      0.32      0.41       340\n",
            "    negative       0.61      0.72      0.66       739\n",
            "     neutral       0.59      0.61      0.60       575\n",
            "\n",
            "    accuracy                           0.60      1654\n",
            "   macro avg       0.60      0.55      0.56      1654\n",
            "weighted avg       0.60      0.60      0.59      1654\n",
            "\n",
            "Rapport de classification pour les données de test:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "    positive       0.51      0.27      0.35       559\n",
            "    negative       0.57      0.80      0.67      1513\n",
            "     neutral       0.71      0.55      0.62      1475\n",
            "\n",
            "    accuracy                           0.61      3547\n",
            "   macro avg       0.60      0.54      0.55      3547\n",
            "weighted avg       0.62      0.61      0.60      3547\n",
            "\n"
          ]
        }
      ],
      "source": [
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "num_epochs = 25\n",
        "\n",
        "# Entraînement du modèle\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(inputs)\n",
        "        loss = loss_function(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "model.eval()\n",
        "def predict(model, data_loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, _ in data_loader:\n",
        "            inputs = torch.nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            predictions.extend(predicted.tolist())\n",
        "    return predictions\n",
        "\n",
        "Y_dev = dataDv['sentiment']\n",
        "Y_test = dataTst['sentiment']\n",
        "\n",
        "Y_dev_pred = predict(model, dev_loader)\n",
        "Y_test_pred = predict(model, test_loader)\n",
        "\n",
        "idx_to_sentiment = {0: 'positive', 1: 'negative', 2: 'neutral'}\n",
        "Y_dev_pred_labels = [idx_to_sentiment[idx] for idx in Y_dev_pred]\n",
        "Y_test_pred_labels = [idx_to_sentiment[idx] for idx in Y_test_pred]\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(\"Rapport de classification pour les données de développement:\")\n",
        "print(classification_report(Y_dev, Y_dev_pred_labels, target_names=['positive', 'negative', 'neutral']))\n",
        "\n",
        "print(\"Rapport de classification pour les données de test:\")\n",
        "print(classification_report(Y_test, Y_test_pred_labels, target_names=['positive', 'negative', 'neutral']))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}